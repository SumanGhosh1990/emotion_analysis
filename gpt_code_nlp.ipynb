{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Load the data\n",
        "file_path = 'bert train data.csv'\n",
        "df = pd.read_csv(file_path, encoding='ISO-8859-1')\n",
        "\n",
        "# Basic text cleaning function\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Convert text to lowercase\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = re.sub(r'\\W+', ' ', text)  # Remove non-alphanumeric characters\n",
        "    return text\n",
        "\n",
        "# Apply text cleaning to the 'Text' column\n",
        "df['Text'] = df['Text'].apply(clean_text)\n",
        "\n",
        "# Encode the sentiment labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['Sentiment'] = label_encoder.fit_transform(df['Sentiment'])\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['Text'], df['Sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n"
      ],
      "metadata": {
        "id": "o7eHsJ9iTypj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "# Convert sparse tensor to dense\n",
        "X_train_tfidf_dense = X_train_tfidf.toarray()\n",
        "X_test_tfidf_dense = X_test_tfidf.toarray()\n",
        "\n",
        "# Build the TF-IDF + Neural Network model\n",
        "model_tfidf = Sequential()\n",
        "model_tfidf.add(Dense(512, input_dim=X_train_tfidf_dense.shape[1], activation='relu'))\n",
        "model_tfidf.add(Dropout(0.5))\n",
        "model_tfidf.add(Dense(256, activation='relu'))\n",
        "model_tfidf.add(Dropout(0.5))\n",
        "model_tfidf.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model_tfidf.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model_tfidf.fit(X_train_tfidf_dense, y_train, epochs=10, batch_size=32, validation_data=(X_test_tfidf_dense, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_tfidf = model_tfidf.predict(X_test_tfidf_dense)\n",
        "y_pred_tfidf_classes = np.argmax(y_pred_tfidf, axis=1)\n",
        "accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf_classes)\n",
        "f1_tfidf = f1_score(y_test, y_pred_tfidf_classes, average='macro')\n",
        "\n",
        "print(f\"TF-IDF Model Accuracy: {accuracy_tfidf}\")\n",
        "print(f\"TF-IDF Model F1 Score: {f1_tfidf}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qo_7MA3Ufwy",
        "outputId": "b47f770e-2dad-4472-ab8c-7c8b9c6df130"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - accuracy: 0.4408 - loss: 1.0950 - val_accuracy: 0.5581 - val_loss: 1.0554\n",
            "Epoch 2/10\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4623 - loss: 1.0584 - val_accuracy: 0.6047 - val_loss: 1.0177\n",
            "Epoch 3/10\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5226 - loss: 1.0396 - val_accuracy: 0.6279 - val_loss: 0.9871\n",
            "Epoch 4/10\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5656 - loss: 0.9948 - val_accuracy: 0.6047 - val_loss: 0.9571\n",
            "Epoch 5/10\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5940 - loss: 0.9460 - val_accuracy: 0.5116 - val_loss: 0.9253\n",
            "Epoch 6/10\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6293 - loss: 0.8866 - val_accuracy: 0.5116 - val_loss: 0.8911\n",
            "Epoch 7/10\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7238 - loss: 0.8071 - val_accuracy: 0.5581 - val_loss: 0.8559\n",
            "Epoch 8/10\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7812 - loss: 0.7216 - val_accuracy: 0.6279 - val_loss: 0.8259\n",
            "Epoch 9/10\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8526 - loss: 0.6174 - val_accuracy: 0.6279 - val_loss: 0.7949\n",
            "Epoch 10/10\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8971 - loss: 0.5166 - val_accuracy: 0.6279 - val_loss: 0.7795\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "TF-IDF Model Accuracy: 0.627906976744186\n",
            "TF-IDF Model F1 Score: 0.7161531279178338\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text for word embeddings\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Pad sequences\n",
        "maxlen = 100\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=maxlen)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=maxlen)\n",
        "\n",
        "# Build the Word Embeddings + LSTM model\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(input_dim=10000, output_dim=128, input_length=maxlen))\n",
        "model_lstm.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model_lstm.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model_lstm.fit(X_train_pad, y_train, epochs=10, batch_size=32, validation_data=(X_test_pad, y_test))\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_lstm = model_lstm.predict(X_test_pad)\n",
        "y_pred_lstm_classes = np.argmax(y_pred_lstm, axis=1)\n",
        "accuracy_lstm = accuracy_score(y_test, y_pred_lstm_classes)\n",
        "f1_lstm = f1_score(y_test, y_pred_lstm_classes, average='macro')\n",
        "\n",
        "print(f\"LSTM Model Accuracy: {accuracy_lstm}\")\n",
        "print(f\"LSTM Model F1 Score: {f1_lstm}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "742NPXPTUoow",
        "outputId": "cd5553d3-36e4-4755-ec34-4a3173db6d9c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 219ms/step - accuracy: 0.3861 - loss: 1.0962 - val_accuracy: 0.5116 - val_loss: 1.0489\n",
            "Epoch 2/10\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step - accuracy: 0.3989 - loss: 1.0653 - val_accuracy: 0.5116 - val_loss: 0.9817\n",
            "Epoch 3/10\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step - accuracy: 0.4786 - loss: 1.0216 - val_accuracy: 0.4651 - val_loss: 0.9745\n",
            "Epoch 4/10\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 269ms/step - accuracy: 0.6444 - loss: 1.0062 - val_accuracy: 0.6279 - val_loss: 0.9614\n",
            "Epoch 5/10\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 292ms/step - accuracy: 0.6366 - loss: 0.9691 - val_accuracy: 0.3721 - val_loss: 0.9395\n",
            "Epoch 6/10\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 154ms/step - accuracy: 0.5005 - loss: 0.8962 - val_accuracy: 0.5116 - val_loss: 0.8660\n",
            "Epoch 7/10\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 153ms/step - accuracy: 0.7900 - loss: 0.7564 - val_accuracy: 0.6744 - val_loss: 0.7714\n",
            "Epoch 8/10\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 151ms/step - accuracy: 0.8803 - loss: 0.6046 - val_accuracy: 0.6512 - val_loss: 0.6902\n",
            "Epoch 9/10\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 153ms/step - accuracy: 0.8535 - loss: 0.5584 - val_accuracy: 0.6512 - val_loss: 0.6953\n",
            "Epoch 10/10\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 149ms/step - accuracy: 0.8939 - loss: 0.4119 - val_accuracy: 0.6977 - val_loss: 0.7022\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 218ms/step\n",
            "LSTM Model Accuracy: 0.6976744186046512\n",
            "LSTM Model F1 Score: 0.7473166946851157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Results summary\n",
        "results = {\n",
        "    \"Model\": [\"TF-IDF + Neural Network\", \"Word Embeddings + LSTM\"],\n",
        "    \"Accuracy\": [accuracy_tfidf, accuracy_lstm],\n",
        "    \"F1 Score\": [f1_tfidf, f1_lstm]\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NW9US9tLTzcy",
        "outputId": "a188cc8b-5b2d-476c-addc-1e97ac0792aa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     Model  Accuracy  F1 Score\n",
            "0  TF-IDF + Neural Network  0.627907  0.716153\n",
            "1   Word Embeddings + LSTM  0.697674  0.747317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZPHnPrfTTzfw"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Hyperparameters\n",
        "input_dim = X_train_tfidf_dense.shape[1]\n",
        "learning_rate = 0.001\n",
        "dropout_rate = 0.5\n",
        "batch_size = 32\n",
        "epochs = 20\n",
        "\n",
        "# Build the TF-IDF + Neural Network model with fine-tuning\n",
        "model_tfidf = Sequential()\n",
        "model_tfidf.add(Dense(512, input_dim=input_dim, activation='relu'))\n",
        "model_tfidf.add(Dropout(dropout_rate))\n",
        "model_tfidf.add(Dense(256, activation='relu'))\n",
        "model_tfidf.add(Dropout(dropout_rate))\n",
        "model_tfidf.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=learning_rate)\n",
        "model_tfidf.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "model_tfidf.fit(X_train_tfidf_dense, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test_tfidf_dense, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_tfidf = model_tfidf.predict(X_test_tfidf_dense)\n",
        "y_pred_tfidf_classes = np.argmax(y_pred_tfidf, axis=1)\n",
        "accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf_classes)\n",
        "f1_tfidf = f1_score(y_test, y_pred_tfidf_classes, average='macro')\n",
        "\n",
        "print(f\"TF-IDF Model Accuracy: {accuracy_tfidf}\")\n",
        "print(f\"TF-IDF Model F1 Score: {f1_tfidf}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bu8_-h0Tzib",
        "outputId": "eecfa237-1bec-419d-f680-4eed30d5a8c0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.4038 - loss: 1.0992 - val_accuracy: 0.4651 - val_loss: 1.0645\n",
            "Epoch 2/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5092 - loss: 1.0679 - val_accuracy: 0.5581 - val_loss: 1.0225\n",
            "Epoch 3/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.5703 - loss: 1.0337 - val_accuracy: 0.5581 - val_loss: 0.9897\n",
            "Epoch 4/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5293 - loss: 0.9953 - val_accuracy: 0.5581 - val_loss: 0.9598\n",
            "Epoch 5/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6090 - loss: 0.9461 - val_accuracy: 0.4884 - val_loss: 0.9339\n",
            "Epoch 6/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.6141 - loss: 0.9133 - val_accuracy: 0.4419 - val_loss: 0.9101\n",
            "Epoch 7/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7205 - loss: 0.8423 - val_accuracy: 0.4651 - val_loss: 0.8727\n",
            "Epoch 8/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8217 - loss: 0.7361 - val_accuracy: 0.5349 - val_loss: 0.8227\n",
            "Epoch 9/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9092 - loss: 0.6305 - val_accuracy: 0.5814 - val_loss: 0.7796\n",
            "Epoch 10/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8965 - loss: 0.5053 - val_accuracy: 0.6047 - val_loss: 0.7402\n",
            "Epoch 11/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.9164 - loss: 0.4012 - val_accuracy: 0.6279 - val_loss: 0.7303\n",
            "Epoch 12/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.9562 - loss: 0.2869 - val_accuracy: 0.6279 - val_loss: 0.7245\n",
            "Epoch 13/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.9533 - loss: 0.2273 - val_accuracy: 0.6047 - val_loss: 0.7571\n",
            "Epoch 14/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9523 - loss: 0.1529 - val_accuracy: 0.5814 - val_loss: 0.8099\n",
            "Epoch 15/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9696 - loss: 0.1507 - val_accuracy: 0.6279 - val_loss: 0.7849\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "TF-IDF Model Accuracy: 0.627906976744186\n",
            "TF-IDF Model F1 Score: 0.6935064935064935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "max_features = 10000\n",
        "embedding_dim = 128\n",
        "maxlen = 100\n",
        "learning_rate = 0.001\n",
        "dropout_rate = 0.2\n",
        "recurrent_dropout_rate = 0.2\n",
        "batch_size = 32\n",
        "epochs = 20\n",
        "\n",
        "# Build the Word Embeddings + LSTM model with fine-tuning\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(input_dim=max_features, output_dim=embedding_dim, input_length=maxlen))\n",
        "model_lstm.add(LSTM(128, dropout=dropout_rate, recurrent_dropout=recurrent_dropout_rate))\n",
        "model_lstm.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=learning_rate)\n",
        "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "model_lstm.fit(X_train_pad, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test_pad, y_test), callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred_lstm = model_lstm.predict(X_test_pad)\n",
        "y_pred_lstm_classes = np.argmax(y_pred_lstm, axis=1)\n",
        "accuracy_lstm = accuracy_score(y_test, y_pred_lstm_classes)\n",
        "f1_lstm = f1_score(y_test, y_pred_lstm_classes, average='macro')\n",
        "\n",
        "print(f\"LSTM Model Accuracy: {accuracy_lstm}\")\n",
        "print(f\"LSTM Model F1 Score: {f1_lstm}\")\n",
        "\n",
        "train_data = pd.DataFrame({\n",
        "    'Text': X_train,\n",
        "    'Actual Sentiment': y_train,\n",
        "    'Predicted Sentiment': y_pred_lstm_train_classes\n",
        "})\n",
        "\n",
        "test_data = pd.DataFrame({\n",
        "    'Text': X_test,\n",
        "    'Actual Sentiment': y_test,\n",
        "    'Predicted Sentiment': y_pred_lstm_classes\n",
        "})\n",
        "\n",
        "# Combine train and test data\n",
        "combined_data = pd.concat([train_data, test_data])\n",
        "\n",
        "# Map integer labels back to original labels\n",
        "combined_data['Actual Sentiment'] = label_encoder.inverse_transform(combined_data['Actual Sentiment'])\n",
        "combined_data['Predicted Sentiment'] = label_encoder.inverse_transform(combined_data['Predicted Sentiment'])\n",
        "\n",
        "# Save the output dataframe to a CSV file\n",
        "output_file_path = 'output_predictions_lstm.csv'  # Update with your desired file path\n",
        "combined_data.to_csv(output_file_path, index=False)\n",
        "print(f\"Predictions saved to {output_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_IGiyckTzlK",
        "outputId": "48a56d51-f7e2-483e-8dbf-c312899f40e5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 218ms/step - accuracy: 0.4188 - loss: 1.0952 - val_accuracy: 0.4186 - val_loss: 1.0634\n",
            "Epoch 2/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 169ms/step - accuracy: 0.5669 - loss: 1.0685 - val_accuracy: 0.5814 - val_loss: 1.0072\n",
            "Epoch 3/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 173ms/step - accuracy: 0.5341 - loss: 1.0462 - val_accuracy: 0.6047 - val_loss: 0.9816\n",
            "Epoch 4/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 152ms/step - accuracy: 0.6115 - loss: 1.0078 - val_accuracy: 0.5814 - val_loss: 0.9855\n",
            "Epoch 5/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 153ms/step - accuracy: 0.6487 - loss: 0.9780 - val_accuracy: 0.5814 - val_loss: 0.9406\n",
            "Epoch 6/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 151ms/step - accuracy: 0.6862 - loss: 0.9000 - val_accuracy: 0.6512 - val_loss: 0.8692\n",
            "Epoch 7/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 151ms/step - accuracy: 0.8070 - loss: 0.7792 - val_accuracy: 0.6744 - val_loss: 0.8330\n",
            "Epoch 8/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 162ms/step - accuracy: 0.8717 - loss: 0.6464 - val_accuracy: 0.6512 - val_loss: 0.7774\n",
            "Epoch 9/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 262ms/step - accuracy: 0.8380 - loss: 0.5394 - val_accuracy: 0.6512 - val_loss: 0.7426\n",
            "Epoch 10/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 287ms/step - accuracy: 0.9040 - loss: 0.3655 - val_accuracy: 0.7209 - val_loss: 0.7116\n",
            "Epoch 11/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 151ms/step - accuracy: 0.8581 - loss: 0.4170 - val_accuracy: 0.6512 - val_loss: 0.7511\n",
            "Epoch 12/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 150ms/step - accuracy: 0.9287 - loss: 0.2596 - val_accuracy: 0.6279 - val_loss: 0.7492\n",
            "Epoch 13/20\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 153ms/step - accuracy: 0.9448 - loss: 0.2605 - val_accuracy: 0.6744 - val_loss: 0.7895\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step\n",
            "LSTM Model Accuracy: 0.7209302325581395\n",
            "LSTM Model F1 Score: 0.7840909090909092\n",
            "Predictions saved to output_predictions_lstm.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Results summary\n",
        "results = {\n",
        "    \"Model\": [\"TF-IDF + Neural Network\", \"Word Embeddings + LSTM\"],\n",
        "    \"Accuracy\": [accuracy_tfidf, accuracy_lstm],\n",
        "    \"F1 Score\": [f1_tfidf, f1_lstm]\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ly_q7ee2Tzny",
        "outputId": "54d07bea-ddc5-4bf3-e102-75d505b16d2c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                     Model  Accuracy  F1 Score\n",
            "0  TF-IDF + Neural Network  0.627907  0.693506\n",
            "1   Word Embeddings + LSTM  0.720930  0.784091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y6AepA_pVZ4w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}